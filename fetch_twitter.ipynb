{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Crawl Data Twitter > 2000 Tweets\n",
        "The crawling process was done using Tweet-Harvest. Written by Helmi Satria on  March 30th 2024. Modified by vizen to better capture the date range.\n",
        "\n"
      ],
      "metadata": {
        "id": "xEWpayxURuUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Twitter Auth Token\n",
        "\n",
        "twitter_auth_token = 'cd964e3842a4a4a397d377aa1e1c48f61e23c40c' # change this auth token"
      ],
      "metadata": {
        "id": "6S00x_f6-GeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4UIL1x21P9rQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb45a2f7-3818-4041-ac8e-3990b0aefff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://deb.nodesource.com/node_20.x nodistro InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,341 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 1,606 kB in 2s (963 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20240203~22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "curl: (23) Failed writing body\n",
            "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 https://deb.nodesource.com/node_20.x nodistro InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "nodejs is already the newest version (20.18.3-1nodesource1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "v20.18.3\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K╔═══════════════════════════════════════════════════════════════════════════════╗\n",
            "║ WARNING: It looks like you are running 'npx playwright install' without first ║\n",
            "║ installing your project's dependencies.                                       ║\n",
            "║                                                                               ║\n",
            "║ To avoid unexpected behavior, please install your dependencies first, and     ║\n",
            "║ then run Playwright's install command:                                        ║\n",
            "║                                                                               ║\n",
            "║     npm install                                                               ║\n",
            "║     npx playwright install                                                    ║\n",
            "║                                                                               ║\n",
            "║ If your project does not yet depend on Playwright, first install the          ║\n",
            "║ applicable npm package (most commonly @playwright/test), and                  ║\n",
            "║ then run Playwright's install command to download the browsers:               ║\n",
            "║                                                                               ║\n",
            "║     npm install @playwright/test                                              ║\n",
            "║     npx playwright install                                                    ║\n",
            "║                                                                               ║\n",
            "╚═══════════════════════════════════════════════════════════════════════════════╝\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libgtk-4.so.1                                    ║\n",
            "║     libgraphene-1.0.so.0                             ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/root/.npm/_npx/e41f203b7505f1fb/node_modules/\u001b[4mplaywright-core\u001b[24m/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/root/.npm/_npx/e41f203b7505f1fb/node_modules/\u001b[4mplaywright-core\u001b[24m/lib/server/registry/index.js:865:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/root/.npm/_npx/e41f203b7505f1fb/node_modules/\u001b[4mplaywright-core\u001b[24m/lib/server/registry/index.js:963:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/root/.npm/_npx/e41f203b7505f1fb/node_modules/\u001b[4mplaywright-core\u001b[24m/lib/server/registry/index.js:952:43)\n",
            "    at async t.<anonymous> (/root/.npm/_npx/e41f203b7505f1fb/node_modules/\u001b[4mplaywright-core\u001b[24m/lib/cli/program.js:122:7)\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "# Import required Python package\n",
        "!pip install pandas\n",
        "\n",
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v\n",
        "\n",
        "!npx playwright install"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6-1EBrlediWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crawl Twitter Data\n",
        "# Tweet retrieval from 2023-11-20 until 2024-02-13\n",
        "\n",
        "import datetime\n",
        "\n",
        "# Define your search parameters\n",
        "search_keyword = '(idx OR goto OR unvr OR adro OR akra OR amrt OR antm OR arto OR asii OR bpca OR bbni OR bbri OR bmri OR brpt OR cpin OR inco OR indf OR itmg OR klbf OR mdka OR medc OR pgas OR ptba OR smgr OR tlkm) lang:id min_faves:0'\n",
        "limit = 10000\n",
        "\n",
        "# Define date range\n",
        "start_date = datetime.datetime(2023, 12, 2)\n",
        "end_date = datetime.datetime(2023, 12, 3)\n",
        "\n",
        "# Loop through dates and crawl tweets day by day\n",
        "current_date = start_date\n",
        "while current_date < end_date:\n",
        "    next_date = current_date + datetime.timedelta(days=1)\n",
        "\n",
        "    # Format dates for filename and search query\n",
        "    date_str_filename = current_date.strftime('%Y%m%d')\n",
        "    date_str_since = current_date.strftime('%Y-%m-%d')\n",
        "    date_str_until = next_date.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Create filename\n",
        "    filename = f'twitter_comments_s{date_str_filename}_u{next_date.strftime(\"%Y%m%d\")}.csv'\n",
        "\n",
        "    # Create search query with date parameters\n",
        "    date_query = f'{search_keyword} since:{date_str_since} until:{date_str_until}'\n",
        "\n",
        "    # Execute tweet-harvest command\n",
        "    print(f\"Collecting tweets for {date_str_since}...\")\n",
        "\n",
        "    # The actual tweet-harvest command\n",
        "    !npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{date_query}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}\n",
        "\n",
        "    # Move to next day\n",
        "    current_date = next_date\n",
        "\n",
        "print(\"Tweet collection completed!\")"
      ],
      "metadata": {
        "id": "LYDR51dJlVlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe55966-50cd-497c-8eeb-752446f8638b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tweets for 2023-12-02...\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\u001b[1m\u001b[32mTweet Harvest [v2.6.1]\u001b[39m\u001b[22m\n",
            "\u001b[1m\u001b[32m\u001b[39m\u001b[22m\n",
            "\u001b[34mResearch by \u001b[39m\u001b[1m\u001b[34mHelmi Satria\u001b[39m\u001b[22m\u001b[34m\u001b[39m\n",
            "\u001b[34mUse it for Educational Purposes only!\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[33mThis script uses Chromium Browser to crawl data from Twitter with \u001b[1myour Twitter auth token\u001b[22m.\u001b[39m\n",
            "\u001b[33mPlease enter your Twitter auth token when prompted.\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m Keep your access token secret! Don't share it with anyone else.\n",
            "\u001b[31m\u001b[1mNote:\u001b[22m\u001b[39m This script only runs on your local device.\n",
            "\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[34mOpening twitter search page...\u001b[39m\n",
            "\u001b[34m\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[33m\u001b[39m\n",
            "\u001b[33mFilling in keywords: (idx OR goto OR unvr OR adro OR akra OR amrt OR antm OR arto OR asii OR bpca OR bbni OR bbri OR bmri OR brpt OR cpin OR inco OR indf OR itmg OR klbf OR mdka OR medc OR pgas OR ptba OR smgr OR tlkm) lang:id min_faves:0 since:2023-12-02 until:2023-12-03\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[90m (2)\u001b[39m\u001b[90m[v2.6.1]\u001b[39m No more tweets found, please check your search criteria and csv file result\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[90m (3)\u001b[39m\u001b[90m (4)\u001b[39m\u001b[90m (5)\u001b[39m\u001b[90m (6)\u001b[39m\u001b[90m (7)\u001b[39m\u001b[90m (8)\u001b[39m\u001b[90m (9)\u001b[39m\u001b[90m (10)\u001b[39m\u001b[90m (11)\u001b[39m\u001b[90m (12)\u001b[39m\u001b[90m (13)\u001b[39m\u001b[90m (14)\u001b[39m\u001b[90m (15)\u001b[39m\u001b[90m (16)\u001b[39m\u001b[90m (17)\u001b[39m\u001b[90m (18)\u001b[39m\u001b[90m (19)\u001b[39m\u001b[90m (20)\u001b[39m\u001b[90m (21)\u001b[39m\u001b[33mNo more tweets found, please check your search criteria and csv file result\u001b[39m\n",
            "\u001b[33mTimeout reached 1 times, making sure again...\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[90m (3)\u001b[39m\u001b[90m (4)\u001b[39m\u001b[90m (5)\u001b[39m\u001b[90m (6)\u001b[39m\u001b[90m (7)\u001b[39m\u001b[90m (8)\u001b[39m\u001b[90m (9)\u001b[39m\u001b[90m (10)\u001b[39m\u001b[90m (11)\u001b[39m\u001b[90m (12)\u001b[39m\u001b[90m (13)\u001b[39m\u001b[90m (14)\u001b[39m\u001b[90m (15)\u001b[39m\u001b[90m (16)\u001b[39m\u001b[90m (17)\u001b[39m\u001b[90m (18)\u001b[39m\u001b[90m (19)\u001b[39m\u001b[90m (20)\u001b[39m\u001b[90m (21)\u001b[39m\u001b[33mNo more tweets found, please check your search criteria and csv file result\u001b[39m\n",
            "\u001b[33mTimeout reached 2 times, making sure again...\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[90m (3)\u001b[39m\u001b[90m (4)\u001b[39m\u001b[90m (5)\u001b[39m\u001b[90m (6)\u001b[39m\u001b[90m (7)\u001b[39m\u001b[90m (8)\u001b[39m\u001b[90m (9)\u001b[39m\u001b[90m (10)\u001b[39m\u001b[90m (11)\u001b[39m\u001b[90m (12)\u001b[39m\u001b[90m (13)\u001b[39m\u001b[90m (14)\u001b[39m\u001b[90m (15)\u001b[39m\u001b[90m (16)\u001b[39m\u001b[90m (17)\u001b[39m\u001b[90m (18)\u001b[39m\u001b[90m (19)\u001b[39m\u001b[90m (20)\u001b[39m\u001b[90m (21)\u001b[39m\u001b[33mNo more tweets found, please check your search criteria and csv file result\u001b[39m\n",
            "\u001b[33mTimeout reached 3 times, making sure again...\u001b[39m\n",
            "\u001b[90m\u001b[39m\n",
            "\u001b[90m-- Scrolling... (1)\u001b[39m\u001b[90m (2)\u001b[39m\u001b[90m (3)\u001b[39m\u001b[90m (4)\u001b[39m\u001b[90m (5)\u001b[39m\u001b[90m (6)\u001b[39m\u001b[90m (7)\u001b[39m\u001b[90m (8)\u001b[39m\u001b[90m (9)\u001b[39m\u001b[90m (10)\u001b[39m\u001b[90m (11)\u001b[39m\u001b[90m (12)\u001b[39m\u001b[90m (13)\u001b[39m\u001b[90m (14)\u001b[39m\u001b[90m (15)\u001b[39m\u001b[90m (16)\u001b[39m\u001b[90m (17)\u001b[39m\u001b[90m (18)\u001b[39m\u001b[90m (19)\u001b[39m\u001b[90m (20)\u001b[39mNo tweets found for the search criteria\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0KTweet collection completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LxqLyXe7Eyl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565a22ab-fbbb-4507-ce83-f6962047ae28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "\n",
        "# Get all CSV files matching the pattern\n",
        "csv_files = sorted(glob.glob(\"tweets-data/twitter_comments_*.csv\"))\n",
        "\n",
        "# Read and merge CSVs\n",
        "df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
        "\n",
        "# Save to a new CSV file\n",
        "df.to_csv(\"merged_twitter_comments.csv\", index=False)\n",
        "\n",
        "print(\"Merged CSV saved as merged_twitter_comments.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmwHGgINKcET",
        "outputId": "63babfb5-6b09-4083-9384-9ec7f0f98627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged CSV saved as merged_twitter_comments.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvAG3hPvQDqk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path = f\"tweets-data/{filename}\"\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path, delimiter=\",\")\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRfDl54waHC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49703a3-4de4-40a5-c041-7cd01093bbc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah tweet dalam dataframe adalah 104.\n"
          ]
        }
      ],
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}